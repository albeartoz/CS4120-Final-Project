{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Steps to run this jupyter notebook\n","1. Make sure you have numpy, pandas, tensorflow, keras, and scikitlearn installed in Jupyter\n","2. Make sure the lyrics.csv file is in the same directory as this jupyter notebook\n","3. Make sure the lyric_prompts.csv file is in the same directory as this jupyter notebook\n","4. Run all code cells in order"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3376,"status":"ok","timestamp":1651668685883,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"eotDbVFaHOO6"},"outputs":[],"source":["# Imports\n","import io\n","import os\n","import sys\n","import csv\n","import string\n","import numpy as np\n","import pandas as pd\n","import tensorflow\n","from collections import Counter\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n","from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n","from keras import Input, Model, backend, utils\n","from keras.layers import *\n","backend.clear_session()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2000,"status":"ok","timestamp":1651668825727,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"2gmbDjtCIBJe","outputId":"86f45dd1-4eb2-4cc1-84fb-16e95f030c19"},"outputs":[],"source":["# Put song data into dataframe\n","\n","translator = str.maketrans('', '', string.punctuation)\n","\n","df = pd.read_csv(\"lyrics.csv\", sep=\"\\t\", engine=\"python\", encoding=\"utf-8\", error_bad_lines=False)\n","# Drops lyrics with NaN as their value\n","df.index.name = 'id'\n","df = df.dropna()\n","\n","df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"elapsed":6834,"status":"ok","timestamp":1651668836432,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"LPNKguWhwcgf","outputId":"72380f42-12b3-447d-9ec2-0bd066b806db"},"outputs":[],"source":["# Text preprocessing to separate lyrics correctly\n","def split_text(x):\n","   text = x['lyrics']\n","   sections = text.split('\\\\n\\\\n')\n","   keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n","   lyrics = str()\n","   single_text = []\n","   res = {}\n","   \n","   for s in sections:\n","       key = s[s.find('[') + 1:s.find(']')].strip()\n","       if ':' in key:\n","           key = key[:key.find(':')]\n","       if key in keys:\n","          single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n","       res['single_text'] =  ' \\n'.join(single_text)\n","   return pd.Series(res)\n","\n","\n","df = df.join( df.apply(split_text, axis=1), lsuffix=\"_Left\", rsuffix=\"_Right\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7019,"status":"ok","timestamp":1651668844908,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"3fSR06ZYNL8q","outputId":"8a5b7226-995d-45a9-9bf2-70ae0abc389f"},"outputs":[],"source":["# Filtering our dataset for common vs. uncommon words\n","text_list = []\n","word_frequencies = Counter()\n","uncommon_words = set()\n","MIN_FREQUENCY = 7\n","MIN_SEQ = 5\n","BATCH_SIZE = 100\n","\n","def extract_text(text):\n","   global text_list\n","   text_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n","\n","df['single_text'].apply( extract_text )\n","print('Total words: ', len(text_list))\n","\n","for w in text_list:\n","   word_frequencies[w] = word_frequencies.get(w, 0) + 1\n","\n","uncommon_words = set([key for key in word_frequencies.keys() if word_frequencies[key] < MIN_FREQUENCY])\n","words = sorted(set([key for key in word_frequencies.keys() if word_frequencies[key] >= MIN_FREQUENCY]))\n","\n","num_words = len(words)\n","\n","word_indices = dict((w, i) for i, w in enumerate(words))\n","indices_word = dict((i, w) for i, w in enumerate(words))\n","\n","print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n","print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n","\n","\n","valid_seqs = []\n","\n","end_seq_words = []\n","\n","for i in range(len(text_list) - MIN_SEQ ):\n","   end_slice = i + MIN_SEQ + 1\n","   if len( set(text_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n","       valid_seqs.append(text_list[i: i + MIN_SEQ])\n","       end_seq_words.append(text_list[i + MIN_SEQ])\n","      \n","\n","print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n","\n","# Split data into training and testing datasets\n","X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.02, random_state=42)\n","\n","print(X_train[2:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":522,"status":"ok","timestamp":1651668847787,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"JjxVqDPu0A1P"},"outputs":[],"source":["# Data generator for fit and evaluate\n","\n","def generator(sentence_list, next_word_list, batch_size):\n","   index = 0\n","   while True:\n","       x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n","       y = np.zeros((batch_size), dtype=np.int32)\n","       for i in range(batch_size):\n","           for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n","               x[i, t] = word_indices[w]\n","           y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n","           index = index + 1\n","       yield x, y\n","\n","\n","# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n","\n","def sample(preds, temperature=1.0):\n","   # helper function to sample an index from a probability array\n","   preds = np.asarray(preds).astype('float64')\n","   preds = np.log(preds) / temperature\n","   exp_preds = np.exp(preds)\n","   preds = exp_preds / np.sum(exp_preds)\n","   probas = np.random.multinomial(1, preds, 1)\n","   return np.argmax(probas)\n","\n","# This generates new verses of 50 words based on the prompts we made\n","def on_epoch_end(epoch, logs):\n","    # Function invoked at end of each epoch. Prints generated text.\n","    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n","    sentences = []\n","    # Randomly pick a seed sequence\n","    with open(\"./lyric_prompts.csv\") as file:\n","        reader = csv.reader(file)\n","        for row in reader:\n","            sentences.append(row[0].lower().replace(',',''))\n","    \n","\n","    for sentence in sentences[1:]:\n","        sentence = sentence.split()\n","        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n","        examples_file.write(' '.join(sentence))\n","\n","        for _ in range(50):\n","            x_pred = np.zeros((1, len(sentence)))\n","            for t, word in enumerate(sentence):\n","                x_pred[0, t] = word_indices[word]\n","\n","            preds = model.predict(x_pred, verbose=0)[0]\n","            next_index = sample(preds, 0.5)\n","            next_word = indices_word[next_index]\n","\n","            sentence = sentence[1:]\n","            sentence.append(next_word)\n","\n","            examples_file.write(\" \"+next_word)\n","        examples_file.write('\\n')\n","    examples_file.write('='*80 + '\\n')\n","    examples_file.flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1651668855403,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"blVSJew30Fpa"},"outputs":[],"source":["def getModel():\n","   print('Build model...')\n","   model = Sequential()\n","   model.add(Embedding(input_dim=len(words), output_dim=1024))\n","   model.add(Bidirectional(LSTM(128)))\n","   model.add(Dense(len(words)))\n","   model.add(Activation('softmax'))\n","   return model\n","\n","def getEncoder():\n","   # int sequences.\n","   enc_inputs = Input(shape=(20,), name='enc_inputs')\n","\n","   \n","   # Embedding lookup and GRU\n","   embedding = Embedding(input_dim=100, output_dim=64)(enc_inputs)\n","   whole_sequence = GRU(4, return_sequences=True)(embedding)\n","\n","   # Query-value attention of shape [batch_size, Tq, filters].\n","   query_value_attention_seq = Attention()([whole_sequence, whole_sequence])\n","\n","   # build encoder model \n","   encoder = Model(enc_inputs, query_value_attention_seq, name='encoder')\n","\n","   return encoder\n","\n","def getDecoder():\n","  # int sequences.\n","  dec_input = Input(shape=(20, 4), name='dec_inputs')\n","\n","  # LSTM\n","  whole_sequence = LSTM(4, return_sequences=True)(dec_input)\n","\n","  # Query-value attention of shape [batch_size, Tq, filters].\n","  query_value_attention_seq = AdditiveAttention()([whole_sequence, dec_input])\n","\n","  # Reduce over the sequence axis to produce encodings of shape\n","  # [batch_size, filters].\n","  query_value_attention = GlobalAveragePooling1D()(query_value_attention_seq)\n","\n","  # classification\n","  dec_output = Dense(1, activation='sigmoid')(query_value_attention)\n","\n","  # build decoder model\n","  decoder = Model(dec_input, dec_output, name='decoder')\n","  return decoder\n","\n","def getAutoEncoder():\n","  encoder = getEncoder()\n","  encoder_init = Input(shape=(20, ))\n","  encoder_output = encoder(encoder_init)\n","  print(encoder_output.shape)\n","\n","  decoder = getDecoder()\n","  decoder_output = decoder(encoder_output)\n","  print(decoder_output.shape)\n","\n","  autoencoder = Model(encoder_init, decoder_output)\n","  return autoencoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_Bodze40OYp","outputId":"d5d816c6-5d10-4ea1-cf4f-590b78a4d630"},"outputs":[],"source":["# model = getAutoEncoder()\n","model = getModel()\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n","\n","\n","file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n","            (len(words), MIN_SEQ, MIN_FREQUENCY)\n","\n","\n","checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n","\n","print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n","\n","callbacks_list = [checkpoint, print_callback, early_stopping]\n","\n","\n","examples_file = open('examples.txt', \"w\")\n","\n","model.fit(generator(X_train, y_train, BATCH_SIZE),\n","                   steps_per_epoch=100, #int(len(valid_seqs)/BATCH_SIZE) + 1,\n","                   epochs=1,\n","                   callbacks=callbacks_list,\n","                   validation_data=generator(X_test, y_train, BATCH_SIZE),\n","                   validation_steps=int(len(y_train)/BATCH_SIZE) + 1)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN0W6WTVLt4fSh552GW90s6","collapsed_sections":[],"name":"song_lyric_generator.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
