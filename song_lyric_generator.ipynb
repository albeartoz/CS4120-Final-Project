{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"song_lyric_generator.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOdd2kZzt6yfzOjJl5yldVX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":142,"metadata":{"id":"eotDbVFaHOO6","executionInfo":{"status":"ok","timestamp":1651430248411,"user_tz":240,"elapsed":135,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"}}},"outputs":[],"source":["# Imports\n","\n","import io\n","import os\n","import sys\n","import string\n","import numpy as np\n","import pandas as pd\n","import tensorflow\n","from collections import Counter\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n","from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n","from keras import Input, Model, backend, utils\n","from keras.layers import *\n","backend.clear_session()"]},{"cell_type":"code","source":["# Put song data into dataframe\n","\n","translator = str.maketrans('', '', string.punctuation)\n","\n","df = pd.read_csv(\"/data/lyrics.csv\", sep=\"\\t\", engine=\"python\", encoding=\"utf-8\", error_bad_lines=False)\n","# Drops lyrics with NaN as their value\n","df.index.name = 'id'\n","df = df.dropna()\n","\n","df.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"2gmbDjtCIBJe","executionInfo":{"status":"ok","timestamp":1651430251424,"user_tz":240,"elapsed":501,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"}},"outputId":"24f76d7c-048a-4dc1-8e24-4485c12f1b7b"},"execution_count":143,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n","\n","\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n","Skipping line 2188: unexpected end of data\n"]},{"output_type":"execute_result","data":{"text/plain":["                   song_id                                             lyrics\n","id                                                                           \n","0   3e9HZxeyfWwjeyPAMmWSSQ  ['[Verse 1]\\nThought I\\'d end up with Sean\\nBu...\n","1   5p7ujcrUXASCNwRaWNHR1C  [\"[Verse 1]\\nFound you when your heart was bro...\n","2   2xLMifQCjDGFmkHkpNLD9h  ['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...\n","4   1rqqCSm0Qe4I9rUvWncaom  [\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...\n","5   0bYg9bo50gSsH3LtXe2SQn  [\"[Intro]\\nI-I-I don't want a lot for Christma..."],"text/html":["\n","  <div id=\"df-801be7d2-07b7-446b-a923-14ed4aa550a8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>song_id</th>\n","      <th>lyrics</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3e9HZxeyfWwjeyPAMmWSSQ</td>\n","      <td>['[Verse 1]\\nThought I\\'d end up with Sean\\nBu...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n","      <td>[\"[Verse 1]\\nFound you when your heart was bro...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n","      <td>['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1rqqCSm0Qe4I9rUvWncaom</td>\n","      <td>[\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0bYg9bo50gSsH3LtXe2SQn</td>\n","      <td>[\"[Intro]\\nI-I-I don't want a lot for Christma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-801be7d2-07b7-446b-a923-14ed4aa550a8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-801be7d2-07b7-446b-a923-14ed4aa550a8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-801be7d2-07b7-446b-a923-14ed4aa550a8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["def split_text(x):\n","\n","   text = x['lyrics']\n","\n","   sections = text.split('\\\\n\\\\n')\n","\n","   keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n","\n","   lyrics = str()\n","\n","   single_text = []\n","\n","   res = {}\n","\n","   \n","   for s in sections:\n","\n","       key = s[s.find('[') + 1:s.find(']')].strip()\n","\n","       if ':' in key:\n","\n","           key = key[:key.find(':')]\n","          \n","\n","       if key in keys:\n","          single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n","\n","       res['single_text'] =  ' \\n'.join(single_text)\n","   return pd.Series(res)\n","\n","\n","df = df.join( df.apply(split_text, axis=1), lsuffix=\"_Left\", rsuffix=\"_Right\")\n","\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"LPNKguWhwcgf","executionInfo":{"status":"ok","timestamp":1651430254797,"user_tz":240,"elapsed":1088,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"}},"outputId":"40671359-af96-49b4-a4ba-298e4f274353"},"execution_count":144,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   song_id                                             lyrics  \\\n","id                                                                              \n","0   3e9HZxeyfWwjeyPAMmWSSQ  ['[Verse 1]\\nThought I\\'d end up with Sean\\nBu...   \n","1   5p7ujcrUXASCNwRaWNHR1C  [\"[Verse 1]\\nFound you when your heart was bro...   \n","2   2xLMifQCjDGFmkHkpNLD9h  ['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...   \n","4   1rqqCSm0Qe4I9rUvWncaom  [\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...   \n","5   0bYg9bo50gSsH3LtXe2SQn  [\"[Intro]\\nI-I-I don't want a lot for Christma...   \n","\n","                                          single_text  \n","id                                                     \n","0   thank you next next \\nthank you next next \\nth...  \n","1   tell me hows it feel sittin up there \\nfeelin ...  \n","2   woo made this here with all the ice on in the ...  \n","4   had to have high high hopes for a living \\nsho...  \n","5   i dont want a lot for christmas \\nthere is jus...  "],"text/html":["\n","  <div id=\"df-9ff8d86a-70c5-4ade-882b-93639ca3e612\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>song_id</th>\n","      <th>lyrics</th>\n","      <th>single_text</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3e9HZxeyfWwjeyPAMmWSSQ</td>\n","      <td>['[Verse 1]\\nThought I\\'d end up with Sean\\nBu...</td>\n","      <td>thank you next next \\nthank you next next \\nth...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n","      <td>[\"[Verse 1]\\nFound you when your heart was bro...</td>\n","      <td>tell me hows it feel sittin up there \\nfeelin ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n","      <td>['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...</td>\n","      <td>woo made this here with all the ice on in the ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1rqqCSm0Qe4I9rUvWncaom</td>\n","      <td>[\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...</td>\n","      <td>had to have high high hopes for a living \\nsho...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0bYg9bo50gSsH3LtXe2SQn</td>\n","      <td>[\"[Intro]\\nI-I-I don't want a lot for Christma...</td>\n","      <td>i dont want a lot for christmas \\nthere is jus...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ff8d86a-70c5-4ade-882b-93639ca3e612')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9ff8d86a-70c5-4ade-882b-93639ca3e612 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9ff8d86a-70c5-4ade-882b-93639ca3e612');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["# Filtering our dataset for common vs. uncommon words\n","text_list = []\n","word_frequencies = Counter()\n","uncommon_words = set()\n","MIN_FREQUENCY = 7\n","MIN_SEQ = 5\n","BATCH_SIZE = 32\n","\n","\n","def extract_text(text):\n","   global text_list\n","   text_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n","\n","\n","df['single_text'].apply( extract_text )\n","\n","print('Total words: ', len(text_list))\n","\n","\n","for w in text_list:\n","\n","   word_frequencies[w] = word_frequencies.get(w, 0) + 1\n","  \n","\n","uncommon_words = set([key for key in word_frequencies.keys() if word_frequencies[key] < MIN_FREQUENCY])\n","\n","words = sorted(set([key for key in word_frequencies.keys() if word_frequencies[key] >= MIN_FREQUENCY]))\n","\n","\n","num_words = len(words)\n","\n","word_indices = dict((w, i) for i, w in enumerate(words))\n","\n","indices_word = dict((i, w) for i, w in enumerate(words))\n","\n","print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n","\n","print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n","\n","\n","valid_seqs = []\n","\n","end_seq_words = []\n","\n","for i in range(len(text_list) - MIN_SEQ ):\n","\n","   end_slice = i + MIN_SEQ + 1\n","\n","   if len( set(text_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n","\n","       valid_seqs.append(text_list[i: i + MIN_SEQ])\n","\n","       end_seq_words.append(text_list[i + MIN_SEQ])\n","      \n","\n","print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.02, random_state=42)\n","\n","print(X_train[2:5])"],"metadata":{"id":"3fSR06ZYNL8q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651430963399,"user_tz":240,"elapsed":5189,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"}},"outputId":"7c4471f6-279d-4cd0-f28c-2737814b3b03"},"execution_count":151,"outputs":[{"output_type":"stream","name":"stdout","text":["Total words:  493654\n","Words with less than 7 appearances: 14398\n","Words with more than 7 appearances: 4252\n","Valid sequences of size 5: 368773\n","[['that', '\\ni', 'had', 'someone', 'tell'], ['see', 'why', '\\nnowadays', 'theres', 'still'], ['\\ni', 'like', 'my', 'kisses', 'down']]\n"]}]},{"cell_type":"code","source":["# Data generator for fit and evaluate\n","\n","def generator(sentence_list, next_word_list, batch_size):\n","\n","   index = 0\n","\n","   while True:\n","\n","       x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n","\n","       y = np.zeros((batch_size), dtype=np.int32)\n","\n","       for i in range(batch_size):\n","\n","           for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n","\n","               x[i, t] = word_indices[w]\n","\n","           y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n","\n","           index = index + 1\n","\n","       yield x, y\n","\n","\n","# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n","\n","def sample(preds, temperature=1.0):\n","\n","   # helper function to sample an index from a probability array\n","\n","   preds = np.asarray(preds).astype('float64')\n","\n","   preds = np.log(preds) / temperature\n","\n","   exp_preds = np.exp(preds)\n","\n","   preds = exp_preds / np.sum(exp_preds)\n","\n","   probas = np.random.multinomial(1, preds, 1)\n","\n","   return np.argmax(probas)\n","\n","\n","def on_epoch_end(epoch, logs):\n","\n","   # Function invoked at end of each epoch. Prints generated text.\n","\n","   examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n","\n","\n","   # Randomly pick a seed sequence\n","\n","   seed_index = np.random.randint(len(X_train+X_test))\n","\n","   seed = (X_train+X_test)[seed_index]\n","\n","\n","   for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n","\n","       sentence = seed\n","\n","       examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n","\n","       examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n","\n","       examples_file.write(' '.join(sentence))\n","\n","\n","       for i in range(50):\n","\n","           x_pred = np.zeros((1, MIN_SEQ))\n","\n","           for t, word in enumerate(sentence):\n","\n","               x_pred[0, t] = word_indices[word]\n","\n","\n","           preds = model.predict(x_pred, verbose=0)[0]\n","\n","           next_index = sample(preds, diversity)\n","\n","           next_word = indices_word[next_index]\n","\n","\n","           sentence = sentence[1:]\n","\n","           sentence.append(next_word)\n","\n","\n","           examples_file.write(\" \"+next_word)\n","\n","       examples_file.write('\\n')\n","\n","   examples_file.write('='*80 + '\\n')\n","\n","   examples_file.flush()"],"metadata":{"id":"JjxVqDPu0A1P","executionInfo":{"status":"ok","timestamp":1651430964074,"user_tz":240,"elapsed":172,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"}}},"execution_count":152,"outputs":[]},{"cell_type":"code","source":["def getModel():\n","\n","   print('Build model...')\n","\n","   model = Sequential()\n","\n","   model.add(Embedding(input_dim=len(words), output_dim=1024))\n","\n","   model.add(Bidirectional(LSTM(128)))\n","\n","   model.add(Dense(len(words)))\n","\n","   model.add(Activation('softmax'))\n","\n","   return model\n","\n","def getEncoder():\n","   # int sequences.\n","   enc_inputs = Input(shape=(20,), name='enc_inputs')\n","\n","   \n","   # Embedding lookup and GRU\n","   embedding = Embedding(input_dim=100, output_dim=64)(enc_inputs)\n","   whole_sequence = GRU(4, return_sequences=True)(embedding)\n","\n","   # Query-value attention of shape [batch_size, Tq, filters].\n","   query_value_attention_seq = Attention()([whole_sequence, whole_sequence])\n","\n","   # build encoder model \n","   encoder = Model(enc_inputs, query_value_attention_seq, name='encoder')\n","\n","   return encoder\n","\n","def getDecoder():\n","  # int sequences.\n","  dec_input = Input(shape=(20, 4), name='dec_inputs')\n","\n","  # LSTM\n","  whole_sequence = LSTM(4, return_sequences=True)(dec_input)\n","\n","  # Query-value attention of shape [batch_size, Tq, filters].\n","  query_value_attention_seq = AdditiveAttention()([whole_sequence, dec_input])\n","\n","  # Reduce over the sequence axis to produce encodings of shape\n","  # [batch_size, filters].\n","  query_value_attention = GlobalAveragePooling1D()(query_value_attention_seq)\n","\n","  # classification\n","  dec_output = Dense(1, activation='sigmoid')(query_value_attention)\n","\n","  # build decoder model\n","  decoder = Model(dec_input, dec_output, name='decoder')\n","  return decoder\n","\n","def getAutoEncoder():\n","  encoder = getEncoder()\n","  encoder_init = Input(shape=(20, ))\n","  encoder_output = encoder(encoder_init)\n","  print(encoder_output.shape)\n","\n","  decoder = getDecoder()\n","  decoder_output = decoder(encoder_output)\n","  print(decoder_output.shape)\n","\n","  autoencoder = Model(encoder_init, decoder_output)\n","  return autoencoder\n"],"metadata":{"id":"blVSJew30Fpa","executionInfo":{"status":"ok","timestamp":1651431080350,"user_tz":240,"elapsed":132,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"}}},"execution_count":156,"outputs":[]},{"cell_type":"code","source":["# model = getAutoEncoder()\n","model = getModel()\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n","\n","\n","file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n","            (len(words), MIN_SEQ, MIN_FREQUENCY)\n","\n","\n","checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n","\n","print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n","\n","callbacks_list = [checkpoint, print_callback, early_stopping]\n","\n","\n","examples_file = open('examples.txt', \"w\")\n","\n","model.fit(generator(X_train, y_train, BATCH_SIZE),\n","\n","                   steps_per_epoch=int(len(valid_seqs)/BATCH_SIZE) + 1,\n","\n","                   epochs=20,\n","\n","                   callbacks=callbacks_list,\n","\n","                   validation_data=generator(X_test, y_train, BATCH_SIZE),\n","\n","                   validation_steps=int(len(y_train)/BATCH_SIZE) + 1)"],"metadata":{"id":"J_Bodze40OYp"},"execution_count":null,"outputs":[]}]}