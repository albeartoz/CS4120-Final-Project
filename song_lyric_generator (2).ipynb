{"cells":[{"cell_type":"markdown","metadata":{"id":"lUz4lDac0FWP"},"source":["# Steps to run this jupyter notebook\n","1. Make sure you have numpy, pandas, tensorflow, keras, and scikitlearn installed in Jupyter\n","2. Make sure the lyrics.csv file is in the same directory as this jupyter notebook\n","3. Make sure the lyric_prompts.csv file is in the same directory as this jupyter notebook\n","4. Run all code cells in order"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7977,"status":"ok","timestamp":1651715494913,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"eotDbVFaHOO6"},"outputs":[],"source":["# Imports\n","import io\n","import os\n","import sys\n","import csv\n","import string\n","import numpy as np\n","import pandas as pd\n","import tensorflow\n","from collections import Counter\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n","from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n","from keras import Input, Model, backend, utils\n","from keras.layers import *\n","backend.clear_session()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"elapsed":891,"status":"error","timestamp":1651715496810,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"2gmbDjtCIBJe","outputId":"7e75501a-afc8-4b18-fe33-828239b4babc"},"outputs":[],"source":["# Put song data into dataframe\n","\n","translator = str.maketrans('', '', string.punctuation)\n","\n","df = pd.read_csv(\"lyrics.csv\", sep=\"\\t\", engine=\"python\", encoding=\"utf-8\", error_bad_lines=False)\n","# Drops lyrics with NaN as their value\n","df.index.name = 'id'\n","df = df.dropna()\n","\n","df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPNKguWhwcgf"},"outputs":[],"source":["# Text preprocessing to add a \"single_text\" field\n","def generate_single_text_field(data):\n","   text = data['lyrics']\n","   sections = text.split('\\\\n\\\\n')\n","   verses = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n","   single_text = []\n","   response = {}\n","   \n","   for section in sections:\n","       verse_tag = section[section.find('[') + 1:section.find(']')].strip()\n","       if ':' in verse_tag:\n","           verse_tag = verse_tag[:verse_tag.find(':')]\n","       if verse_tag in verses:\n","          single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in section[section.find(']')+1:].split('\\\\n') if len(x) > 1]\n","       response['single_text'] =  ' \\n'.join(single_text)\n","   return pd.Series(response)\n","\n","\n","df = df.join( df.apply(generate_single_text_field, axis=1), lsuffix=\"_Left\", rsuffix=\"_Right\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fSR06ZYNL8q"},"outputs":[],"source":["# Constants\n","BATCH_SIZE = 100\n","MIN_SEQ = 5\n","MIN_FREQUENCY = 7\n","text_list = []\n","word_frequencies = Counter()\n","uncommon_words = set()\n","valid_sequences = []\n","end_of_sequence_words = []\n","\n","# Filtering our dataset for common vs. uncommon words\n","\n","def fill_text_list(text):\n","   global text_list\n","   text_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n","\n","df['single_text'].apply( fill_text_list )\n","print('Total words: ', len(text_list))\n","\n","for word in text_list:\n","   word_frequencies[word] = word_frequencies.get(word, 0) + 1\n","\n","uncommon_words = set([key for key in word_frequencies.keys() if word_frequencies[key] < MIN_FREQUENCY])\n","words = sorted(set([key for key in word_frequencies.keys() if word_frequencies[key] >= MIN_FREQUENCY]))\n","\n","num_words = len(words)\n","\n","word_indices = dict((word, index) for index, word in enumerate(words))\n","indices_word = dict((index, word) for index, word in enumerate(words))\n","\n","\n","\n","for i in range(len(text_list) - MIN_SEQ ):\n","   end_slice = i + MIN_SEQ + 1\n","   if len( set(text_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n","       valid_sequences.append(text_list[i: i + MIN_SEQ])\n","       end_of_sequence_words.append(text_list[i + MIN_SEQ])\n","      \n","# Split data into training and testing datasets\n","X_train, X_test, y_train, y_test = train_test_split(valid_sequences, end_of_sequence_words, test_size=0.02, random_state=42)\n","print(X_train[2:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjxVqDPu0A1P"},"outputs":[],"source":["# Data generator for fit and evaluate methods\n","# I had no experience with making generator functions so I used one I found online\n","def generator(sentence_list, next_word_list, batch_size):\n","   index = 0\n","   while True:\n","       x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n","       y = np.zeros((batch_size), dtype=np.int32)\n","       for i in range(batch_size):\n","           for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n","               x[i, t] = word_indices[w]\n","           y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n","           index = index + 1\n","       yield x, y\n","\n","\n","# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n","def sample(preds, temperature=1.0):\n","   # helper function to sample an index from a probability array\n","   preds = np.asarray(preds).astype('float64')\n","   preds = np.log(preds) / temperature\n","   exp_preds = np.exp(preds)\n","   preds = exp_preds / np.sum(exp_preds)\n","   probas = np.random.multinomial(1, preds, 1)\n","   return np.argmax(probas)\n","\n","# This generates new verses of 50 words based on the prompts we made\n","def on_epoch_end(epoch, logs):\n","    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n","    sentences = []\n","    with open(\"./lyric_prompts.csv\") as file:\n","        reader = csv.reader(file)\n","        for row in reader:\n","            sentences.append(row[0].lower().replace(',',''))\n","    \n","\n","    for sentence in sentences[1:]:\n","        sentence = sentence.split()\n","        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n","        examples_file.write(' '.join(sentence))\n","\n","        for i in range(50):\n","            x_pred = np.zeros((1, len(sentence)))\n","            for t, word in enumerate(sentence):\n","                x_pred[0, t] = word_indices[word]\n","\n","            preds = model.predict(x_pred, verbose=0)[0]\n","            next_index = sample(preds, 0.5)\n","            next_word = indices_word[next_index]\n","\n","            sentence = sentence[1:]\n","            sentence.append(next_word)\n","\n","            examples_file.write(\" \"+next_word)\n","        examples_file.write('\\n')\n","    examples_file.write('='*80 + '\\n')\n","    examples_file.flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blVSJew30Fpa"},"outputs":[],"source":["def getModel():\n","   print('Build model...')\n","   model = Sequential()\n","   model.add(Embedding(input_dim=len(words), output_dim=1024))\n","   model.add(Bidirectional(LSTM(128)))\n","   model.add(Dense(len(words)))\n","   model.add(Activation('softmax'))\n","   return model\n","\n","\n","# Commented out code from trying to implement attention mechanism\n","\n","# def getEncoder():\n","#    # int sequences.\n","#    enc_inputs = Input(shape=(20,), name='enc_inputs')\n","\n","   \n","#    # Embedding lookup and GRU\n","#    embedding = Embedding(input_dim=100, output_dim=64)(enc_inputs)\n","#    whole_sequence = GRU(4, return_sequences=True)(embedding)\n","\n","#    # Query-value attention of shape [batch_size, Tq, filters].\n","#    query_value_attention_seq = Attention()([whole_sequence, whole_sequence])\n","\n","#    # build encoder model \n","#    encoder = Model(enc_inputs, query_value_attention_seq, name='encoder')\n","\n","#    return encoder\n","\n","# def getDecoder():\n","#   # int sequences.\n","#   dec_input = Input(shape=(20, 4), name='dec_inputs')\n","\n","#   # LSTM\n","#   whole_sequence = LSTM(4, return_sequences=True)(dec_input)\n","\n","#   # Query-value attention of shape [batch_size, Tq, filters].\n","#   query_value_attention_seq = AdditiveAttention()([whole_sequence, dec_input])\n","\n","#   # Reduce over the sequence axis to produce encodings of shape\n","#   # [batch_size, filters].\n","#   query_value_attention = GlobalAveragePooling1D()(query_value_attention_seq)\n","\n","#   # classification\n","#   dec_output = Dense(1, activation='sigmoid')(query_value_attention)\n","\n","#   # build decoder model\n","#   decoder = Model(dec_input, dec_output, name='decoder')\n","#   return decoder\n","\n","# def getAutoEncoder():\n","#   encoder = getEncoder()\n","#   encoder_init = Input(shape=(20, ))\n","#   encoder_output = encoder(encoder_init)\n","#   print(encoder_output.shape)\n","\n","#   decoder = getDecoder()\n","#   decoder_output = decoder(encoder_output)\n","#   print(decoder_output.shape)\n","\n","#   autoencoder = Model(encoder_init, decoder_output)\n","#   return autoencoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_Bodze40OYp"},"outputs":[],"source":["# model = getAutoEncoder()\n","model = getModel()\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n","\n","\n","# Code from online to save metrics to a file\n","file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n","            (len(words), MIN_SEQ, MIN_FREQUENCY)\n","\n","\n","checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n","\n","print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n","\n","callbacks_list = [checkpoint, print_callback, early_stopping]\n","\n","\n","examples_file = open('examples.txt', \"w\")\n","\n","model.fit(generator(X_train, y_train, BATCH_SIZE),\n","                   steps_per_epoch=100, #int(len(valid_seqs)/BATCH_SIZE) + 1,\n","                   epochs=1,\n","                   callbacks=callbacks_list,\n","                   validation_data=generator(X_test, y_train, BATCH_SIZE),\n","                   validation_steps=int(len(y_train)/BATCH_SIZE) + 1)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"song_lyric_generator (2).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
